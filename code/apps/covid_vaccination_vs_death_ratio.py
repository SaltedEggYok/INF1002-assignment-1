# -*- coding: utf-8 -*-
"""Covid Vaccination Vs Death Ratio (LR).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EnVFPSGDP3lQMhT6w5ZIR7Xc1M1CDr4Z

# *Simple Linear Regression*
This notebook studies the relationship between the number of vaccinated personnel and death using different machine-learning methods.

### *Problem Understanding*
Covid-19 has become a global pandemic and is affecting all nations worldwide. The emerging threat posed by the Covid-19 pandemic has affected every aspect of our lives, including our active lifestyle, economy, social fabric, and healthcare sector. The government and other countries have been working together to fight this tough battle. As IT professionals, my team is tasked to develop one intuitive and intelligent tool for users to understand better the situation of Covid-19 in Singapore and other countries.

### *Analytic approach*
The number of deaths is a continuous variable. We can use the simple linear regression method so that the number of deaths is our dependent variable and the vaccination percentage is our independent variable.

### *Data requirements and Data Collection*
We need a table of vaccination statistics from all over the world to understand the relationship. We also need to know the population of every country in the world and the number of deaths from COVID-19.

First of all, we <font color=green>import</font> the libraries required in this project:
"""

# Commented out IPython magic to ensure Python compatibility.
# Import Skcit-Learn methods we want to use in this notebook
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import r2_score
from sklearn import linear_model
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# Import useful Libraries
import pylab as pl
import plotly.express as px
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import datetime
# %matplotlib inline

mortality = pd.read_csv('../csv/covid-vaccination-vs-death_ratio.csv')
#mortality is the name of the dataframe here
mortality.head()
#This is a dataset with 26622 lines of data regarding total vaccinations, people vaccinated, and new deaths by country and date

mortality.describe()

mortality.info()

mortality["country"].unique()

"""# Evaluation
To measure the accuracy of a regression model, we compare the actual and projected values. Evaluation metrics play an important part in the development of a model since they give insight into areas that need to be improved. There are several model assessment metrics; in this case, we'll use MSE to determine the accuracy of our model based on the test set:

Mean absolute error (MAE) is a measure of errors between paired observations expressing the same phenomenon. Examples of Y versus X include comparisons of predicted versus observed, subsequent time versus initial time, and one technique of measurement versus an alternative technique of measurement. MAE is calculated as:

![image.png](attachment:image.png)

The mean squared error (MSE) or mean squared deviation (MSD) of an estimator (of a procedure for estimating an unobserved quantity) measures the average of the squares of the errorsâ€”that is, the average squared difference between the estimated values and the actual value. MSE is a risk function, corresponding to the expected value of the squared error loss.The fact that MSE is almost always strictly positive (and not zero) is because of randomness or because the estimator does not account for information that could produce a more accurate estimate:

![image-2.png](attachment:image-2.png)

Root Mean Squared Error (RMSE) is the standard deviation of the residuals (prediction errors):

![image-3.png](attachment:image-3.png)

R-squared is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model. Whereas correlation explains the strength of the relationship between an independent and dependent variable, R-squared explains to what extent the variance of one variable explains the variance of the second variable. So, if the R^2 of a model is 0.50, then approximately half of the observed variation can be explained by the model's inputs.

![image-4.png](attachment:image-4.png)

![image-5.png](attachment:image-5.png)
"""

# We put the data from Spain in the mortality variable .
mydf = mortality[mortality.country == "Denmark"]

# Create an object to build a linear regression model from Scikit-Learn Library. 
regr = linear_model.LinearRegression()

# We define training variables via Numpy in arrays .
train_x = np.asanyarray(mydf[['ratio']])
train_y = np.asanyarray(mydf[['New_deaths']])

# Using fit(x, y) method of Scikit-Learn object, we fit the model on the training variables.
regr.fit (train_x, train_y)

# The coefficients and Intercept of this Simple Linear Regression
print(f'Coefficients: {regr.coef_[0][0]}')
print(f'Intercept: {regr.intercept_[0]}')

# Now it is time to draw the line we want using Coefficients and Intercept. 
plt.scatter(mydf.ratio, mydf.New_deaths,  color='orange') 
XX = train_x
YY = regr.intercept_[0] + regr.coef_[0][0]*train_x   # y = Intercept + (Coeff * VaccinationRate)
# Plotting Regression Line
plt.plot(XX, YY, color='blue')
plt.title("Denmark")
plt.xlabel("Vaccination rate (%) ")
plt.ylabel("New deaths")
plt.show()

# Calculate Predicted values by this model
test_x = np.asanyarray(mydf[['ratio']])
test_y = np.asanyarray(mydf[['New_deaths']])
predict_y = regr.predict(train_x)

# Using Predicted values to mesure Error of this model
# Mean absolute error
MAE = np.mean(np.absolute(predict_y  - test_y))  
print(f"Mean absolute error: {MAE:.2f}")

# Mean squared error
MSE =  np.mean((predict_y  - test_y) ** 2)
print(f"Residual sum of squares (MSE): {MSE:.2f}")

# R2-score
r2 = r2_score(test_y , predict_y)
print(f"R2-score: {r2:.2f}")

# We put the data from Spain in the mortality variable .
mydf = mortality[mortality.country == "Spain"]

# Create an object to build a linear regression model from Scikit-Learn Library. 
regr = linear_model.LinearRegression()

# We define training variables via Numpy in arrays .
train_x = np.asanyarray(mydf[['ratio']])
train_y = np.asanyarray(mydf[['New_deaths']])

# Using fit(x, y) method of Scikit-Learn object, we fit the model on the training variables.
regr.fit (train_x, train_y)

# The coefficients and Intercept of this Simple Linear Regression
print(f'Coefficients: {regr.coef_[0][0]}')
print(f'Intercept: {regr.intercept_[0]}')

# Now it is time to draw the line we want using Coefficients and Intercept. 
plt.scatter(mydf.ratio, mydf.New_deaths,  color='orange') 
XX = train_x
YY = regr.intercept_[0] + regr.coef_[0][0]*train_x   # y = Intercept + (Coeff * VaccinationRate)

# Plotting Regression Line
plt.plot(XX, YY, color='blue')
plt.title("Spain")
plt.xlabel("Vaccination rate (%) ")
plt.ylabel("New deaths")
plt.show()

# Calculate Predicted values by this model
test_x = np.asanyarray(mydf[['ratio']])
test_y = np.asanyarray(mydf[['New_deaths']])
predict_y = regr.predict(train_x)

# Using Predicted values to mesure Error of this model
# Mean absolute error
MAE = np.mean(np.absolute(predict_y  - test_y))  
print(f"Mean absolute error: {MAE:.2f}")

# Mean squared error
MSE =  np.mean((predict_y  - test_y) ** 2)
print(f"Residual sum of squares (MSE): {MSE:.2f}")

# R2-score
r2 = r2_score(test_y , predict_y)
print(f"R2-score: {r2:.2f}")

"""## Takeaways:

- the **R2-score: 0.19** for Spain and **R2-score: 0.01** for Denmark. This shows a very poor relationship the accounts for the variation;

- This data comparison would have been better suited for with Polynomial regression modeling as the plot graph is **curved**. Therefore, **Polynomial regression** may be preferable

- It seems that as more people are fully vaccinated, there are fewer deaths.

- It seems that this data was taken from before covid's peak amount of deaths before people were vaccinated through the deaths spiking and lowering. **Time** should be factored in as a constraint to shape the data

# Polynomial Regression

Polynomial regression is a form of regression analysis in which the relationship between the independent variable x and the dependent variable y is modelled as an nth degree polynomial in x. The goal of regression analysis is to model the expected value of a dependent variable y in terms of the value of an independent variable (or vector of independent variables) x.

![image.png](attachment:image.png)


Let test out whether polynomial regression would be reliable to use this data.

We define the **```plot_vaccine_mortality()```** function, which has three variables: ```country_name```, ```dataframe```, and ```degree```.
"""

def plot_vaccine_mortality(country_name, df, degree=2):
    """
    This function receives the dataset and the name of the country and dgree then divides
    data into two parts the [test] and [train]. And a polynomial regression model applies to 
    training data.Then evaluate the model using test data and prints the results. 
    how to use:
    >>> plot_vaccine_mortality("Italy", df, 8)
    """
    #---------------------------
    print(f"{country_name:-^80}")
    # Store country data in a variable 
    mydf = mortality[mortality.country == country_name]
    
    # Divide data randomly into two test and training sections 
    msk = np.random.rand(len(mydf)) < .8
    train = mydf[msk]
    test = mydf[~msk]
    
    # Identify the dependent(y) and independent variables(x) in the train dataframe
    train_x = np.asanyarray(train[['ratio']])
    train_y = np.asanyarray(train[['New_deaths']])
    
    # Identify the dependent(y) and non-dependent(x) variables in the test dataframe
    test_x = np.asanyarray(train[['ratio']])
    test_y = np.asanyarray(train[['New_deaths']])
    
    # Generate polynomial and interaction features Object with our desired degree  
    poly = PolynomialFeatures(degree=degree)
    
    # In this section, we make a number of variables with different degrees from 
    # independent variables(x) to use them in a multiple regression model.
    train_x_poly = poly.fit_transform(train_x)
    
    # Make the model 
    clf = linear_model.LinearRegression()
    train_y_ = clf.fit(train_x_poly, train_y)
    
    # Print The coefficients
    print ('Coefficients: ')
    for i, c in enumerate(clf.coef_[0]):
        if i: print(f"{c:->22.10f} * X^{i}")
            
    # Print The Intercept    
    print ('Intercept: ',clf.intercept_[0])
    
    # Constructing a scatterplot using train data with random color
    plt.scatter(train.ratio, train.New_deaths,  color= np.random.rand(3,))
    
    # Set the X axis using numpy:   np.arange(start, end, interval)
    XX = np.arange(train_x[0], train_x[-1], 0.1)
    
    # Set the Y axis using intercept and coefficients that we found in previous steps
    YY = clf.intercept_[0] 
    for d in range(1,degree+1):
        YY += clf.coef_[0][d]*np.power(XX, d)
        
    # On the previous scatterplot, we fit the regression model with red color. 
    plt.plot(XX, YY, '-r' )
    plt.title(country_name)
    plt.xlabel("Vaccination rate (%) ")
    plt.ylabel("New deaths")
    plt.show()
    
    # Now it's time to evaluate the model we build 
    # Calculate Predicted values by this model
    test_x_poly = poly.fit_transform(test_x)
    predict_y = clf.predict(test_x_poly)
    
    # Using Predicted values to mesure Error of this model
    # Mean absolute error
    MAE = np.mean(np.absolute(predict_y - test_y))  
    print(f"Mean absolute error: {MAE:.2f}")
    
    # Mean squared error
    MSE =  np.mean((predict_y - test_y) ** 2)
    print(f"Residual sum of squares (MSE): {MSE:.2f}")
    
    # R2-score
    r2 = r2_score(test_y, predict_y)
    print(f"R2-score: {r2:.2f}")
    #---------------------------
    print("-"*80)

"""### Now we return to Spain's country's data and try to apply a polynomial regression model with degree of 3."""

plot_vaccine_mortality("Spain", mortality, 3)

"""Despite the fact that R2-score has improved from 0.19 to 0.62, it can still be better. Therefore, we increase the degree of regression to create a model with the least error. (Note that because the data is randomly divided into test and train part, their value may be slightly different in different runs.)

|| 1 | 2 | 3 | 4 | 5 | **6** | 7 | 8 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| **R2-Score** | 0.20 | 0.59 | 0.63 | 0.79 | 0.77| **0.87** | 0.85 | 0.84 | 

- The polynomial regression of Spain data generates the least amount of error in the **sixth degree**

# **Plotting other countries in sixth degree**
"""

plot_vaccine_mortality("Singapore", mortality, 6)

plot_vaccine_mortality("The United Kingdom", mortality, 6)

plot_vaccine_mortality("United States of America", mortality, 6)

plot_vaccine_mortality("Denmark", mortality, 6)

plot_vaccine_mortality("India", mortality, 8)

plot_vaccine_mortality("Australia", mortality, 8)

"""## Takeaways:

- the **R2-score** for every country has improved. Showing a strong correlation between the deaths and vaccination rate.

- It seems that as more people are fully vaccinated, there are fewer deaths.

- The data was taken from before covid's peak amount of deaths before people were vaccinated through the deaths spiking and lowering. **Time** should be factored in as a constraint to shape the data

"""

